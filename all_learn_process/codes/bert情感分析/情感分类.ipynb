{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入需要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T08:35:42.713276Z",
     "start_time": "2020-12-15T08:35:40.612891Z"
    }
   },
   "outputs": [],
   "source": [
    "#part2: bert feature-base\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as tfs\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T08:35:51.822929Z",
     "start_time": "2020-12-15T08:35:51.796001Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.tsv', delimiter='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T08:35:53.286016Z",
     "start_time": "2020-12-15T08:35:53.268064Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>just about the best straight up , old school h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>men in black ii achieves ultimate insignifican...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>decent but dull</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  1\n",
       "4995  just about the best straight up , old school h...  1\n",
       "441   men in black ii achieves ultimate insignifican...  0\n",
       "458                                     decent but dull  0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T08:35:55.320577Z",
     "start_time": "2020-12-15T08:35:55.315591Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6920"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T08:35:56.604149Z",
     "start_time": "2020-12-15T08:35:56.598164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (3000, 2)\n"
     ]
    }
   ],
   "source": [
    "train_set = train_df[:3000]\n",
    "\n",
    "print(\"Train set shape:\", train_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T08:35:57.922622Z",
     "start_time": "2020-12-15T08:35:57.908661Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1565\n",
       "0    1435\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[1].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用BERT进行特征抽取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，我们利用BERT对数据集进行特征抽取，即把输入数据经过BERT模型，来获取输入数据的特征，这些特征包含了整个句子的信息，是语境层面的。这种做法类似于EMLo的特征抽取。需要注意的是，这里并没有使用到BERT的微调，因为BERT并不参与后面的训练，仅仅进行特征抽取操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T08:36:53.985766Z",
     "start_time": "2020-12-15T08:36:48.165325Z"
    }
   },
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (tfs.BertModel, tfs.BertTokenizer, 'bert-base-uncased')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用预训练好的\"bert-base-uncased\"模型参数进行处理，采用的模型是BertModel，采用的分词器是BertTokenizer。由于我们的输入句子是英文句子，所以需要先分词；然后把单词映射成词汇表的索引，再喂给模型。实际上Bert的分词操作，不是以传统的单词为单位的，而是以wordpiece为单位，这是比单词更细粒度的单位。我们执行以下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T08:39:54.744603Z",
     "start_time": "2020-12-15T08:39:54.740613Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert-base-uncased'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T08:39:49.481674Z",
     "start_time": "2020-12-15T08:39:49.477682Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T08:40:18.714531Z",
     "start_time": "2020-12-15T08:40:17.252440Z"
    }
   },
   "outputs": [],
   "source": [
    "train_tokenized = train_set[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T08:40:32.193503Z",
     "start_time": "2020-12-15T08:40:32.181535Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       a stirring , funny and finally transporting re...\n",
       "1       apparently reassembled from the cutting room f...\n",
       "2       they presume their audience wo n't sit still f...\n",
       "3       this is a visually stunning rumination on love...\n",
       "4       jonathan parker 's bartleby should have been t...\n",
       "                              ...                        \n",
       "2995    but this new jangle of noise , mayhem and stup...\n",
       "2996               darkly funny and frequently insightful\n",
       "2997                                  formuliac , but fun\n",
       "2998    there 's some good material in their story abo...\n",
       "2999    damon brings the proper conviction to his role...\n",
       "Name: 0, Length: 3000, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T08:40:40.555152Z",
     "start_time": "2020-12-15T08:40:40.549168Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [101, 1037, 18385, 1010, 6057, 1998, 2633, 182...\n",
       "1       [101, 4593, 2128, 27241, 23931, 2013, 1996, 62...\n",
       "2       [101, 2027, 3653, 23545, 2037, 4378, 24185, 10...\n",
       "3       [101, 2023, 2003, 1037, 17453, 14726, 19379, 1...\n",
       "4       [101, 5655, 6262, 1005, 1055, 12075, 2571, 376...\n",
       "                              ...                        \n",
       "2995    [101, 2021, 2023, 2047, 23769, 2571, 1997, 500...\n",
       "2996     [101, 27148, 6057, 1998, 4703, 12369, 3993, 102]\n",
       "2997      [101, 2433, 20922, 2278, 1010, 2021, 4569, 102]\n",
       "2998    [101, 2045, 1005, 1055, 2070, 2204, 3430, 1999...\n",
       "2999    [101, 11317, 7545, 1996, 5372, 10652, 2000, 20...\n",
       "Name: 0, Length: 3000, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，为了提升训练速度，我们需要把句子都处理成同一个长度，即常见的pad操作，我们在短的句子末尾添加一系列的[PAD]符号："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T08:41:03.849886Z",
     "start_time": "2020-12-15T08:41:03.828941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set shape: (3000, 66)\n"
     ]
    }
   ],
   "source": [
    "train_max_len = 0\n",
    "for i in train_tokenized.values:\n",
    "    if len(i) > train_max_len:\n",
    "        train_max_len = len(i)\n",
    "\n",
    "train_padded = np.array([i + [0] * (train_max_len-len(i)) for i in train_tokenized.values])\n",
    "print(\"train set shape:\",train_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T08:41:10.984815Z",
     "start_time": "2020-12-15T08:41:10.980825Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T08:41:52.078971Z",
     "start_time": "2020-12-15T08:41:52.074981Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101,  1037, 18385, ...,     0,     0,     0],\n",
       "       [  101,  4593,  2128, ...,     0,     0,     0],\n",
       "       [  101,  2027,  3653, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101,  2433, 20922, ...,     0,     0,     0],\n",
       "       [  101,  2045,  1005, ...,     0,     0,     0],\n",
       "       [  101, 11317,  7545, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们还需要让模型知道，哪些词是不用处理的，即上面我们添加的[PAD]符号："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T08:44:37.758115Z",
     "start_time": "2020-12-15T08:44:37.750135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  101  1037 18385  1010  6057  1998  2633 18276  2128 16603  1997  5053\n",
      "  1998  1996  6841  1998  5687  5469  3152   102     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_padded[0])\n",
    "train_attention_mask = np.where(train_padded != 0, 1, 0)\n",
    "print(train_attention_mask[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过上面一系列步骤的处理，此时输入数据已经可以正确被Bert模型接收并处理了，我们直接进行特征的输出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T08:49:18.102759Z",
     "start_time": "2020-12-15T08:45:18.132196Z"
    }
   },
   "outputs": [],
   "source": [
    "# 训练集\n",
    "train_input_ids = torch.tensor(train_padded).long()\n",
    "train_attention_mask = torch.tensor(train_attention_mask).long()\n",
    "with torch.no_grad():\n",
    "    train_last_hidden_states = model(train_input_ids, attention_mask=train_attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来看以下Bert模型给我们的输出是什么样的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T08:51:38.203274Z",
     "start_time": "2020-12-15T08:51:38.199285Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_last_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T08:53:14.525807Z",
     "start_time": "2020-12-15T08:53:14.517828Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7915, -0.3743, -0.7874,  ..., -0.5462, -0.5542,  0.7294],\n",
       "        [-0.9337, -0.3978, -0.7455,  ..., -0.7045, -0.6715,  0.9190],\n",
       "        [-0.6180, -0.3858, -0.9156,  ..., -0.7709, -0.6184,  0.7713],\n",
       "        ...,\n",
       "        [-0.7152, -0.2524, -0.0280,  ..., -0.2456, -0.5770,  0.8065],\n",
       "        [-0.6578, -0.4616, -0.9024,  ..., -0.7255, -0.6244,  0.7785],\n",
       "        [-0.8487, -0.3505, -0.6283,  ..., -0.0346, -0.6380,  0.8275]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_last_hidden_states[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T08:51:10.525261Z",
     "start_time": "2020-12-15T08:51:10.510298Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3000, 66, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_last_hidden_states[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一维的是样本数量，第二维的是序列长度，第三维是特征数量。也就是说，Bert对于我们的每一个位置的输入，都会输出一个对应的特征向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 切分数据成训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T08:58:19.106444Z",
     "start_time": "2020-12-15T08:58:19.093479Z"
    }
   },
   "outputs": [],
   "source": [
    "train_features = train_last_hidden_states[0][:,0,:].numpy()\n",
    "train_labels = train_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T08:58:19.929245Z",
     "start_time": "2020-12-15T08:58:19.925255Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 768)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意：我们使用[:,0,:]来提取序列第一个位置的输出向量，因为第一个位置是[CLS]，比起其他位置，该向量应该更具有代表性，蕴含了整个句子的信息。紧接着，我们利用sklearn库的方法来把数据集切分成训练集和测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T09:02:36.031066Z",
     "start_time": "2020-12-15T09:02:36.006133Z"
    }
   },
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T09:02:57.865702Z",
     "start_time": "2020-12-15T09:02:57.861713Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2250, 768)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T09:03:13.738275Z",
     "start_time": "2020-12-15T09:03:13.734286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用逻辑回归进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一部分，我们使用sklearn的逻辑回归模块对我们的训练集进行拟合，最后在测试集上进行评价："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T09:03:24.469590Z",
     "start_time": "2020-12-15T09:03:24.214272Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T09:03:29.473216Z",
     "start_time": "2020-12-15T09:03:29.461248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.848"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过逻辑回归模型的拟合，其准确率达到了83.06，分类效果还不错。那么，我们还能进一步提升吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用BERT基于微调的方式进行建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上一部分，我们利用了Bert抽取特征的能力进行建模，提取了Bert的输出特征，再输入给一个线性层以预测。但Bert本身的不参与模型的训练。现在我们采取另一种方式，即fine-tuned，Bert与线性层一起参与训练，反向传播会更新二者的参数，使得Bert模型更加适合这个分类任务。那么，让我们开始吧~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T09:19:08.611672Z",
     "start_time": "2020-12-15T09:19:08.602696Z"
    }
   },
   "outputs": [],
   "source": [
    "#part 2 - bert fine-tuned\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import transformers as tfs\n",
    "import math\n",
    "\n",
    "class BertClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertClassificationModel, self).__init__()   \n",
    "        model_class, tokenizer_class, pretrained_weights = (tfs.BertModel, tfs.BertTokenizer, 'bert-base-uncased')         \n",
    "        self.tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "        self.bert = model_class.from_pretrained(pretrained_weights)\n",
    "        self.dense = nn.Linear(768, 2)  #bert默认的隐藏单元数是768， 输出单元是2，表示二分类\n",
    "        \n",
    "    def forward(self, batch_sentences):\n",
    "        #print(batch_sentences[4])\n",
    "        batch_tokenized = self.tokenizer.batch_encode_plus(batch_sentences, add_special_tokens=True,\n",
    "                                max_length=66, pad_to_max_length=True, truncation=True)      #tokenize、add special token、pad\n",
    "        input_ids = torch.tensor(batch_tokenized['input_ids'])\n",
    "        attention_mask = torch.tensor(batch_tokenized['attention_mask'])\n",
    "        bert_output = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        bert_cls_hidden_state = bert_output[0][:,0,:]       #提取[CLS]对应的隐藏状态\n",
    "        linear_output = self.dense(bert_cls_hidden_state)\n",
    "        return linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T09:08:12.518642Z",
     "start_time": "2020-12-15T09:08:12.508669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method batch_encode_plus in module transformers.tokenization_utils_base:\n",
      "\n",
      "batch_encode_plus(batch_text_or_text_pairs:Union[List[str], List[Tuple[str, str]], List[List[str]], List[Tuple[List[str], List[str]]], List[List[int]], List[Tuple[List[int], List[int]]]], add_special_tokens:bool=True, padding:Union[bool, str, transformers.tokenization_utils_base.PaddingStrategy]=False, truncation:Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy]=False, max_length:Union[int, NoneType]=None, stride:int=0, is_split_into_words:bool=False, pad_to_multiple_of:Union[int, NoneType]=None, return_tensors:Union[str, transformers.tokenization_utils_base.TensorType, NoneType]=None, return_token_type_ids:Union[bool, NoneType]=None, return_attention_mask:Union[bool, NoneType]=None, return_overflowing_tokens:bool=False, return_special_tokens_mask:bool=False, return_offsets_mapping:bool=False, return_length:bool=False, verbose:bool=True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding method of transformers.models.bert.tokenization_bert.BertTokenizer instance\n",
      "    Tokenize and prepare for the model a list of sequences or a list of pairs of sequences.\n",
      "    \n",
      "    .. warning::\n",
      "        This method is deprecated, ``__call__`` should be used instead.\n",
      "    \n",
      "    Args:\n",
      "        batch_text_or_text_pairs (:obj:`List[str]`, :obj:`List[Tuple[str, str]]`, :obj:`List[List[str]]`, :obj:`List[Tuple[List[str], List[str]]]`, and for not-fast tokenizers, also :obj:`List[List[int]]`, :obj:`List[Tuple[List[int], List[int]]]`):\n",
      "            Batch of sequences or pair of sequences to be encoded. This can be a list of\n",
      "            string/string-sequences/int-sequences or a list of pair of string/string-sequences/int-sequence (see\n",
      "            details in ``encode_plus``).\n",
      "    \n",
      "        add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      "            Whether or not to encode the sequences with the special tokens relative to their model.\n",
      "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      "            Activates and controls padding. Accepts the following values:\n",
      "    \n",
      "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      "              single sequence if provided).\n",
      "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      "              maximum acceptable input length for the model if that argument is not provided.\n",
      "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      "              different lengths).\n",
      "        truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      "            Activates and controls truncation. Accepts the following values:\n",
      "    \n",
      "            * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      "              :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      "              provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      "              if a pair of sequences (or a batch of pairs) is provided.\n",
      "            * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      "              the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      "              truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      "            * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      "              to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      "              truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      "            * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      "              sequence lengths greater than the model maximum admissible input size).\n",
      "        max_length (:obj:`int`, `optional`):\n",
      "            Controls the maximum length to use by one of the truncation/padding parameters.\n",
      "    \n",
      "            If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      "            length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      "            input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      "        stride (:obj:`int`, `optional`, defaults to 0):\n",
      "            If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      "            :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      "            returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      "            argument defines the number of overlapping tokens.\n",
      "        is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      "            Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer\n",
      "            will skip the pre-tokenization step. This is useful for NER or token classification.\n",
      "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
      "            If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      "            the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      "        return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):\n",
      "            If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      "    \n",
      "            * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      "            * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      "            * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      "    \n",
      "        return_token_type_ids (:obj:`bool`, `optional`):\n",
      "            Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      "            the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      "    \n",
      "            `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      "        return_attention_mask (:obj:`bool`, `optional`):\n",
      "            Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      "            to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      "    \n",
      "            `What are attention masks? <../glossary.html#attention-mask>`__\n",
      "        return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      "            Whether or not to return overflowing token sequences.\n",
      "        return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      "            Whether or not to return special tokens mask information.\n",
      "        return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      "            Whether or not to return :obj:`(char_start, char_end)` for each token.\n",
      "    \n",
      "            This is only available on fast tokenizers inheriting from\n",
      "            :class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise\n",
      "            :obj:`NotImplementedError`.\n",
      "        return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      "            Whether or not to return the lengths of the encoded inputs.\n",
      "        verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      "            Whether or not to print more information and warnings.\n",
      "        **kwargs: passed to the :obj:`self.tokenize()` method\n",
      "    \n",
      "    Return:\n",
      "        :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      "    \n",
      "        - **input_ids** -- List of token ids to be fed to a model.\n",
      "    \n",
      "          `What are input IDs? <../glossary.html#input-ids>`__\n",
      "    \n",
      "        - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`\n",
      "          or if `\"token_type_ids\"` is in :obj:`self.model_input_names`).\n",
      "    \n",
      "          `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      "    \n",
      "        - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      "          :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in :obj:`self.model_input_names`).\n",
      "    \n",
      "          `What are attention masks? <../glossary.html#attention-mask>`__\n",
      "    \n",
      "        - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and\n",
      "          :obj:`return_overflowing_tokens=True`).\n",
      "        - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and\n",
      "          :obj:`return_overflowing_tokens=True`).\n",
      "        - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      "          regular sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).\n",
      "        - **length** -- The length of the inputs (when :obj:`return_length=True`)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenizer.batch_encode_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T09:35:28.870784Z",
     "start_time": "2020-12-15T09:35:28.851835Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import transformers as tfs\n",
    "import math\n",
    "\n",
    "class BertClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertClassificationModel, self).__init__()\n",
    "        model_class, tokenizer_class, pretrained_weights = (tfs.BertModel, tfs.BertTokenizer,'bert-base-uncased')\n",
    "        self.tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "        self.bert = model_class.from_pretrained(pretrained_weights)\n",
    "        self.dense = nn.Linear(768, 2)\n",
    "        \n",
    "    def forward(self, batch_sentences):\n",
    "        batch_tokenized = self.tokenizer.batch_encode_plus(batch_sentences,add_special_tokens=True,\n",
    "                          max_length = 66, pad_to_max_length = True, truncation = True)\n",
    "        input_ids = torch.tensor(batch_tokenized['input_ids'])\n",
    "        attention_mask = torch.tensor(batch_tokenized['attention_mask'])\n",
    "        bert_output = self.bert(input_ids, attention_mask = attention_mask)\n",
    "        bert_cls_hidden_state = bert_output[0][:,0,:] #提取[CLS]对应的隐藏状态\n",
    "        linear_output = self.dense(bert_cls_hidden_state)\n",
    "        return linear_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型很简单，关键代码都在上面注释了。其主要构成是在bert模型的[CLS]输出位置接上一个线性层，用以预测句子的分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据分批"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们对原来的数据集进行一些改造，分成batch_size为64大小的数据集，以便模型进行批量梯度下降。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T09:32:01.890040Z",
     "start_time": "2020-12-15T09:32:01.880066Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = train_set[0].values\n",
    "targets = train_set[1].values\n",
    "train_inputs, test_inputs, train_targets, test_targets = train_test_split(sentences, targets)\n",
    "\n",
    "batch_size = 64\n",
    "batch_count = int(len(train_inputs) / batch_size)\n",
    "batch_train_inputs, batch_train_targets = [], []\n",
    "for i in range(batch_count):\n",
    "    batch_train_inputs.append(train_inputs[i*batch_size : (i+1)*batch_size])\n",
    "    batch_train_targets.append(train_targets[i*batch_size : (i+1)*batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T09:58:32.229101Z",
     "start_time": "2020-12-15T09:37:29.803536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次迭代:\n",
      "Batch: 10, Loss: 1.5290\n",
      "Batch: 20, Loss: 0.7111\n",
      "Batch: 30, Loss: 0.7068\n",
      "第1次迭代:\n",
      "Batch: 10, Loss: 0.6958\n",
      "Batch: 20, Loss: 0.7138\n",
      "Batch: 30, Loss: 0.7080\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "epochs = 2\n",
    "lr = 0.01\n",
    "print_every_batch = 10\n",
    "bert_classifier_model = BertClassificationModel()\n",
    "#optimizer = optim.SGD(bert_classifier_model.parameters(), lr=lr)\n",
    "optimizer = optim.Adam(bert_classifier_model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print_avg_loss = 0\n",
    "    print('第{}次迭代:'.format(epoch))\n",
    "    for i in range(batch_count):\n",
    "        inputs = batch_train_inputs[i] #输入向量\n",
    "        labels = torch.tensor(batch_train_targets[i]) #输入向量标签\n",
    "        optimizer.zero_grad() #梯度清零\n",
    "        outputs = bert_classifier_model(inputs) #前向传播\n",
    "        loss = criterion(outputs, labels) #计算损失\n",
    "        loss.backward() #反向传播\n",
    "        optimizer.step()#更新梯度\n",
    "        \n",
    "        print_avg_loss += loss.item() #同一批次内的损失相加\n",
    "        if i % print_every_batch == (print_every_batch-1):\n",
    "            print(\"Batch: %d, Loss: %.4f\" % ((i+1), print_avg_loss/print_every_batch))\n",
    "            print_avg_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T10:07:23.141446Z",
     "start_time": "2020-12-15T10:05:19.757249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 51.73%\n"
     ]
    }
   ],
   "source": [
    "# eval the trained model\n",
    "total = len(test_inputs)\n",
    "hit = 0\n",
    "with torch.no_grad():\n",
    "    for i in range(total):\n",
    "        outputs = bert_classifier_model([test_inputs[i]])\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        if predicted == test_targets[i]:\n",
    "            hit += 1\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (hit / total * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T01:34:22.680882Z",
     "start_time": "2020-12-11T01:34:22.676891Z"
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，通过微调的方式来建模，经过3个轮次的训练后，模型的准确率达到了90.53%，比起基于特征的建模方式有了较大提升。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorch] *",
   "language": "python",
   "name": "conda-env-.conda-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
